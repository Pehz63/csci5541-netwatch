<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Appropriateness Filtering for Corporate RAG Database Storage | CSCI 5541 (Fall 2025)</title>
  <link rel="stylesheet" href="./files/bulma.min.css" />
  <link rel="stylesheet" href="./files/styles.css" />
  <link rel="preconnect" href="https://fonts.gstatic.com/" />
  <link href="./files/css2" rel="stylesheet" />
  <link href="./files/css" rel="stylesheet" />
  <base href="." target="_blank" />
  <style>
    .wrapper{max-width:980px;margin:0 auto;padding:24px}
    .sys-img img{width:100%;height:auto}
    .authors-wrapper{display:flex;gap:18px;flex-wrap:wrap}
    .author-container{display:flex;flex-direction:column;align-items:center}
    .author-image img{width:96px;height:96px;border-radius:50%;object-fit:cover;background:#eee}
    .taglist{display:flex;gap:8px;flex-wrap:wrap;margin:6px 0 0}
    .kbd{background:#f5f5f5;border:1px solid #ddd;border-bottom-width:2px;border-radius:6px;padding:2px 6px;font-family:monospace}
    .sys-img.teaser img{width:auto;max-height:420px;display:block;margin:0 auto}
    table{margin:0 auto}
    caption{caption-side:bottom;padding-top:8px}
    .note{font-size:.95rem;color:#444}
    .callout{background:#f7fbff;border:1px solid #cfe8ff;border-radius:10px;padding:12px}
  </style>
</head>
<body>
  <div class="wrapper">
    <h1 style="font-family:'Lato',sans-serif;">Appropriateness Filtering of Claims for Corporate RAG Knowledge Base Storage</h1>
    <h4 style="font-family:'Lato',sans-serif;">Fall 2025 · CSCI 5541 NLP · University of Minnesota</h4>
    <h4 style="font-family:'Lato',sans-serif;">Team: NetWatch</h4>

    <p class="tagline">Demonstration of methodology to increase enterprise RAG quality by <em>pre-embedding filtering</em> of claim-level content, pulled from informal communications such as email or Slack.</p>

    <div class="authors-wrapper">
      <div class="author-container">
        <div class="author-image"><img src="./files/Alex_B.png" alt="A. Berg" /></div>
        <p><strong>Alex Berg</strong><br/><small><a href="mailto:ber00221@umn.edu">ber00221@umn.edu</a></small></p>
      </div>
      <div class="author-container">
        <div class="author-image"><img src="./files/Zeph_J.jpg" alt="Z. Johnson" /></div>
        <p><strong>Zephaniah Johnson</strong><br/><small><a href="mailto:joh15514@umn.edu">joh15514@umn.edu</a></small></p>
      </div>
      <div class="author-container">
        <div class="author-image"><img src="./files/Alex_S.jpg" alt="A. Slinger" /></div>
        <p><strong>Alex Slinger</strong><br/><small><a href="mailto:sling031@umn.edu">sling031@umn.edu</a></small></p>
      </div>
      <div class="author-container">
        <div class="author-image"><img src="./files/Sunder_S.png" alt="S. Subramanian" /></div>
        <p><strong>Sunder Subramanian</strong><br/><small><a href="mailto:subra287@umn.edu">subra287@umn.edu</a></small></p>
      </div>
    </div>

    <br />

    <div class="authors-wrapper">
      <div class="publication-links">
        <span class="link-block">
          <a href="https://drive.google.com/file/d/1obnKf6Ce1kFNnq7v_GVe8zXkgA1gKC6E/view?usp=sharing" class="external-link button is-normal is-rounded is-dark is-outlined"><span>Proposal Report</span></a>
        </span>
        <span class="link-block">
          <a href="https://drive.google.com/file/d/1YnDOI3AoUNUrtaROnrkHDFnjNrRWlU79/view?usp=sharing" class="external-link button is-normal is-rounded is-dark is-outlined"><span>Code files (ZIP)</span></a>
        </span>
      </div>
    </div>
  </div>

<div class="wrapper">
    <hr />

    <h2 id="abstract">Abstract</h2>
    <p>
      Corporate RAG systems often ingest unstructured email and chat that mix high-value institutional
      knowledge with personal, sensitive, or low-information content. We propose a pre-embedding
      filtering framework that decomposes emails into granular claims and classifies each for
      appropriateness using modular risk criteria (relevance, confidentiality, sarcasm, opinion,
      toxicity, personal vs business, temporal) before embedding.
    </p>
    <p>
      Using an Enron-derived claim dataset with human annotations, we compare single-pass and
      multilayer LLM-based filters against human “keep/discard” labels. The best configurations reach
      balanced accuracies around 73–74%, remove most irrelevant claims while preserving the majority
      of important ones, reduce vector store size by ~81%, and prevent filtered content from being
      retrieved by downstream RAG agents.
    </p>
    <hr />

    <h2 id="teaser">Teaser Figure</h2>
    <p>Pipeline: ingestion → claim extraction → risk filtering → embedding → retrieval &amp; QA.</p>
    <p class="sys-img teaser">
      <img src="./files/full_workflow.png" alt="Project pipeline diagram" />
    </p>
    <h3 id="teaser-notes">Notes</h3>
    <p class="note">
      In the multilayer design, each claim receives per-category risk scores that drive a
      keep/discard decision and can later be used to tune retrieval by risk type.
    </p>
    <hr />

    <h2 id="introduction">Introduction / Motivation</h2>
    <p>
      RAG is increasingly used to expose internal knowledge, but the underlying vector stores often
      contain informal communications where business-relevant and inappropriate content co-exist in
      the same message. Prior work largely optimizes retrieval and reranking after embedding, rather
      than controlling what enters the database. We instead study claim-level, ingestion-time
      filtering of corporate emails and ask whether it can simultaneously improve retrieval quality
      and reduce privacy and compliance risk.
    </p>
    <hr />

    <h2 id="approach">Approach</h2>
    <div class="callout">
      <ol>
        <li>
          <b>Data & ingestion.</b> We use the Berkeley-labelled Enron email subset (~1,700 emails) as
          our primary corpus and construct a claim-level dataset via LLM-based extraction.
        </li>
        <li>
          <b>Claim extraction.</b> A DSPy-driven prompt runs three steps—selection, disambiguation,
          and decomposition—to produce standalone, context-rich claims that can be embedded and
          filtered independently.
        </li>
        <li>
          <b>Filters.</b> We implement (a) a single-pass filter that considers all seven risk
          dimensions at once and outputs keep/discard, and (b) a multilayer filter that assigns 0–5
          risk scores per category and aggregates them into a final label.
        </li>
        <li>
          <b>Storage & retrieval.</b> Kept claims are embedded with OpenAI
          <code>text-embedding-3-large</code> and stored in Pinecone. A LangChain-based RAG agent
          retrieves nearest-neighbour claims to answer user questions via a Gradio UI.
        </li>
        <li>
          <b>Evaluation.</b> We compare model outputs to human labels using balanced accuracy and
          confusion matrices, analyze inter-annotator agreement, and measure effects on index size and
          latency versus a “vanilla RAG” baseline that stores all sentence chunks.
        </li>
      </ol>
    </div>

    <h3 id="impl">Implementation Snapshot</h3>
    <ul>
      <li><b>LLMs:</b> GPT-5.1 (primary), GPT-5-mini (ablations), GPT-5.2 (comparison).</li>
      <li><b>Prompt orchestration:</b> DSPy for signatures, chain-of-thought, and few-shot optimization.</li>
      <li><b>Embeddings / DB:</b> <code>text-embedding-3-large</code> into a Pinecone index.</li>
      <li><b>RAG agent / UI:</b> LangChain agent + Gradio <code>ChatInterface</code>.</li>
    </ul>
    <hr />

    <h2 id="filters">Filter Modules</h2>
    <p>
      Each claim is evaluated along seven dimensions: relevance, confidentiality, sarcasm, opinion,
      toxicity, personal vs business, and temporal. The single-pass model reasons over all criteria
      jointly to produce keep/discard, while the multilayer model scores each category separately and
      then aggregates. The latter exposes more control over policy (e.g., tightening confidentiality
      while relaxing temporal strictness) at the cost of higher runtime.
    </p>

    <hr />
    <h2 id="claimify">Claim Extraction</h2>
    <p>
      The claim extraction prompt identifies verifiable statements, rewrites them so each is
      self-contained, and decomposes oversaturated sentences into independent claims where
      appropriate. This produces a numbered list of claims per email, each with an ID and text
      string. These claim units are the basic objects for filtering, embedding, and evaluation.
    </p>

    <hr />
    <h2 id="data">Data & Annotation</h2>
    <p>
      Starting from the Berkeley Enron corpus (~1,700 emails), we apply the claim extractor and then
      manually annotate a subset of claims. Four annotators assign a value-of-information score, a
      risk score, a binary keep/discard label (keep if value &gt; risk), and applicable risk
      categories. About 20% of claims are double-coded. Cohen’s kappa averages around 0.25 across
      overlapping subsets, with most pairs near 0.3–0.35 and one lower outlier, indicating that
      “appropriateness” is subjective and that model performance is bounded by this label noise.
    </p>
    <hr />

    <h2 id="results">Results</h2>

    <p>
      We report single-pass and multilayer filter performance, development tradeoffs, and the impact
      of design choices such as chain-of-thought, example type, and model size. Vanilla RAG
      (storing everything) serves as a 50% balanced-accuracy baseline for keep/discard decisions.
    </p>

    <h3>Inter-Annotator Agreement</h3>
    <p>
      Inter-annotator agreement is modest (κ ≈ 0.25 overall), particularly for subjective categories
      like temporal vs durable relevance and opinion. This suggests that mid-70% balanced accuracy is
      close to the attainable ceiling when evaluated against a single rater.
    </p>

    <!-- ============================= -->
    <!-- Table 1 — Single-pass model   -->
    <!-- ============================= -->

    <h3>Ablation Performance – Single-Pass Filter</h3>

    <table class="table is-fullwidth is-striped">
      <thead>
        <tr>
          <th style="text-align:center">Model Version</th>
          <th style="text-align:center">Balanced Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><b>Final single-layer model</b> (1 filter, GPT-5.1, optimized few-shot)</td>
          <td style="text-align:center">73.0%</td>
        </tr>
        <tr>
          <td>→ manual True/False examples</td>
          <td style="text-align:center">72.7%</td>
        </tr>
        <tr>
          <td>→ removed False counter examples</td>
          <td style="text-align:center">71.2%</td>
        </tr>
        <tr>
          <td>→ removed all examples (zero-shot)</td>
          <td style="text-align:center">71.4%</td>
        </tr>
        <tr>
          <td>→ removed CoT prompt</td>
          <td style="text-align:center">70.8%</td>
        </tr>
        <tr>
          <td>→ downgraded model to GPT-5-mini</td>
          <td style="text-align:center">61.0%</td>
        </tr>
        <tr>
          <td><b>Vanilla RAG baseline</b> (no filtering)</td>
          <td style="text-align:center">50.0%</td>
        </tr>
      </tbody>
      <caption>
        Table 1. Single-layer filter ablations showing the impact of examples, chain-of-thought, and
        LLM choice on claim-level keep/discard accuracy.
      </caption>
    </table>

    <hr/>

    <!-- ============================= -->
    <!-- Table 2 — Multilayer model    -->
    <!-- ============================= -->

    <h3>Ablation Performance – Multilayer Filter</h3>

    <table class="table is-fullwidth is-striped">
      <thead>
        <tr>
          <th style="text-align:center">Model Version</th>
          <th style="text-align:center">Balanced Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><b>Final multilayer model</b> (7 filters, GPT-5.1, manual examples)</td>
          <td style="text-align:center">71.8%</td>
        </tr>
        <tr>
          <td>→ LLM True/False examples</td>
          <td style="text-align:center">69.8%</td>
        </tr>
        <tr>
          <td>→ removed False counter examples</td>
          <td style="text-align:center">67.8%</td>
        </tr>
        <tr>
          <td>→ removed all examples (zero-shot)</td>
          <td style="text-align:center">63.6%</td>
        </tr>
        <tr>
          <td>→ removed CoT prompt</td>
          <td style="text-align:center">73.1%</td>
        </tr>
        <tr>
          <td>→ downgraded model to GPT-5-mini</td>
          <td style="text-align:center">59.0%</td>
        </tr>
        <tr>
          <td><b>Highest scoring multilayer model</b> (7 filters, GPT-5.1, No-CoT, LLM few-shot)</td>
          <td style="text-align:center">73.9%</td>
        </tr>
        <tr>
          <td><b>Vanilla RAG baseline</b> (no filtering)</td>
          <td style="text-align:center">50.0%</td>
        </tr>
      </tbody>
      <caption>
        Table 2. Multilayer filter ablations, including the highest scoring configuration and the
        effect of examples and chain-of-thought.
      </caption>
    </table>

    <hr/>

    <!-- ============================================= -->
    <!-- Table 3 — Model Dev Time vs Accuracy          -->
    <!-- ============================================= -->

    <h3>Model Development Accuracy &amp; Compute Time</h3>

    <table class="table is-fullwidth is-striped">
      <thead>
        <tr>
          <th style="text-align:center">Model Version</th>
          <th style="text-align:center">Balanced Accuracy</th>
          <th style="text-align:center">Calc. Time</th>
          <th style="text-align:center">Complexity</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><b>Final single-layer model</b> (1 filter, GPT-5.1, optimized few-shot)</td>
          <td style="text-align:center">73.0%</td>
          <td style="text-align:center">30 mins</td>
          <td style="text-align:center">Very High</td>
        </tr>
        <tr>
          <td>→ LLM True/False examples</td>
          <td style="text-align:center">72.7%</td>
          <td style="text-align:center">20 mins</td>
          <td style="text-align:center">Mid</td>
        </tr>
        <tr>
          <td>→ removed False counter examples</td>
          <td style="text-align:center">71.2%</td>
          <td style="text-align:center">24 mins</td>
          <td style="text-align:center">Mid</td>
        </tr>
        <tr>
          <td>→ removed all examples (zero-shot)</td>
          <td style="text-align:center">71.4%</td>
          <td style="text-align:center">15 mins</td>
          <td style="text-align:center">Low</td>
        </tr>
        <tr>
          <td>→ removed CoT prompt</td>
          <td style="text-align:center">70.8%</td>
          <td style="text-align:center">2 mins</td>
          <td style="text-align:center">Very Low</td>
        </tr>
        <tr>
          <td>→ downgraded model to GPT-5-mini</td>
          <td style="text-align:center">61.0%</td>
          <td style="text-align:center">17 mins</td>
          <td style="text-align:center">Very Low</td>
        </tr>
        <tr>
          <td><b>Final multilayer model</b> (7 filters, GPT-5.1, optimized few-shot)</td>
          <td style="text-align:center">67.8%</td>
          <td style="text-align:center">132 mins</td>
          <td style="text-align:center">Very High</td>
        </tr>
        <tr>
          <td>→ manual True/False examples</td>
          <td style="text-align:center">71.8%</td>
          <td style="text-align:center">101 mins</td>
          <td style="text-align:center">High</td>
        </tr>
        <tr>
          <td>→ LLM True/False examples</td>
          <td style="text-align:center">69.8%</td>
          <td style="text-align:center">52 mins</td>
          <td style="text-align:center">High</td>
        </tr>
        <tr>
          <td>→ removed False counter examples</td>
          <td style="text-align:center">67.8%</td>
          <td style="text-align:center">45 mins</td>
          <td style="text-align:center">High</td>
        </tr>
        <tr>
          <td>→ removed all examples (zero-shot)</td>
          <td style="text-align:center">68.2%</td>
          <td style="text-align:center">51 mins</td>
          <td style="text-align:center">Mid</td>
        </tr>
        <tr>
          <td>→ removed CoT prompt</td>
          <td style="text-align:center">70.1%</td>
          <td style="text-align:center">16 mins</td>
          <td style="text-align:center">Mid</td>
        </tr>
        <tr>
          <td>→ downgraded model to GPT-5-mini</td>
          <td style="text-align:center">59.0%</td>
          <td style="text-align:center">146 mins</td>
          <td style="text-align:center">Mid</td>
        </tr>
        <tr>
          <td><b>Highest scoring multilayer model</b> (7 filters, GPT-5.1, No-CoT, few-shot)</td>
          <td style="text-align:center">73.9%</td>
          <td style="text-align:center">20 mins</td>
          <td style="text-align:center">High</td>
        </tr>
        <tr>
          <td><b>GPT-5.2 multilayer comparison</b> (7 filters, GPT-5.2, optimized few-shot)</td>
          <td style="text-align:center">66.9%</td>
          <td style="text-align:center">73 mins</td>
          <td style="text-align:center">Very High</td>
        </tr>
        <tr>
          <td>→ manual True/False examples</td>
          <td style="text-align:center">65.0%</td>
          <td style="text-align:center">62 mins</td>
          <td style="text-align:center">High</td>
        </tr>
        <tr>
          <td>→ removed CoT prompt, no examples</td>
          <td style="text-align:center">61.6%</td>
          <td style="text-align:center">16 mins</td>
          <td style="text-align:center">Mid</td>
        </tr>
      </tbody>
      <caption>
        Table 3. Development-time tradeoffs for single-layer and multilayer filters, showing the
        accuracy, runtime, and complexity consequences of different prompt and model choices.
      </caption>
    </table>

    <hr />

    <h2 id="conclusion">Conclusion &amp; Future Work</h2>

    <p>
      Pre-embedding filtering of informal corporate communications can substantially improve the
      safety and efficiency of enterprise RAG. Our claim-level pipeline removes most irrelevant
      content while preserving the majority of important claims, operates near the human agreement
      ceiling, and shrinks the vector store by roughly 81%, which also speeds up retrieval. Qualitative
      probing confirms that discarded information is no longer recoverable by downstream RAG agents.
    </p>

    <h3>Future Work</h3>
    <p>
      Future expansions of would be expected to expand and refine claim-level annotations, extend the pipeline to chat and meeting
      transcripts, explore redaction alongside filtering, reduce latency via lighter models, and make
      retrieval explicitly risk-aware by using multilayer scores directly during search and generation.
    </p>

    <hr />
  </div>
</body>
</html>





